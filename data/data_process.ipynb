{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "import pandas as pd\n",
    "\n",
    "comp_fin = pd.read_excel('dataset/FS_Combas.xlsx',skiprows=[0,2]) # 上市公司财务报表：2020-2023\n",
    "comp_basic = pd.read_excel('dataset/HLD_Copro.xlsx',skiprows=[1,2]) # 上市公司基本情况 （行业/注册资本）\n",
    "volitions = pd.read_excel('dataset/STK_Violation_Main.xlsx',skiprows=[1,2]) # 违规记录\n",
    "shareholders = pd.read_excel('dataset/HLD_Shareholders.xlsx',skiprows=[1,2]) # 股东\n",
    "pri_basic = pd.read_excel('dataset/PRI_Basic.xlsx',skiprows=[1,2]) # 民营企业情况 （行业/区域）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不考虑时序信息的财务信息\n",
    "\n",
    "comp_fin_single = comp_fin.drop_duplicates(subset=['证券代码'],keep='last')\n",
    "comp_fin_single = comp_fin_single.drop(['证券简称','统计截止日期','报表类型'], axis=1)\n",
    "comp_fin_single = comp_fin_single.fillna(value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.公司节点\n",
    "# 2.股东节点\n",
    "# 3.构建关系 投资关系cc cp pc \n",
    "set_fin_comps = set(comp_fin['证券代码'].to_list()) # 包含财务信息的节点数量：5322\n",
    "\n",
    "set_comps = set(comp_basic['Stkcd'].to_list()) # 上市公司的节点数量：5510\n",
    "\n",
    "set_selected_comps = set_fin_comps & set_comps # 节点数量：5321 具有财务信息的上市公司\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_temp_nodes = [] # (code,name)\n",
    "big_comp_names = []\n",
    "\n",
    "for idx, row in comp_basic.iterrows():\n",
    "    code = row['Stkcd']\n",
    "    if code in set_selected_comps:\n",
    "        set_temp_nodes.append((code,row['Conme']))\n",
    "        big_comp_names.append(row['Conme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = shareholders[['Stkcd','S0301a']]\n",
    "df1_single_10 = df1.groupby('Stkcd').head(10)\n",
    "\n",
    "temp_list = []\n",
    "# temp_dict = {}\n",
    "temp_p_code = 1000000\n",
    "temp_c_code = 2000000\n",
    "\n",
    "for index,row in df1_single_10.iterrows():\n",
    "    name = row['S0301a']\n",
    "    cname = name.split(\"-\",1)[0]\n",
    "    if cname not in big_comp_names: \n",
    "        if cname not in temp_list:\n",
    "            if len(cname) > 3:\n",
    "                set_temp_nodes.append((temp_c_code,cname))\n",
    "                # temp_dict[cname] = temp_c_code\n",
    "                temp_list.append(cname)\n",
    "                temp_c_code += 1 \n",
    "            else:\n",
    "                set_temp_nodes.append((temp_p_code,cname))  \n",
    "                # temp_dict[cname] = temp_c_code  \n",
    "                temp_list.append(cname)\n",
    "                temp_p_code += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_code2name = {}\n",
    "mapper_name2code = {}\n",
    "mapper_idx2code = {}\n",
    "mapper_code2idx = {}\n",
    "mapper_idx2name = {}\n",
    "mapper_name2idx = {}\n",
    "\n",
    "for index, item in enumerate(set_temp_nodes):\n",
    "    mapper_idx2code[index] = item[0]\n",
    "    mapper_code2idx[item[0]] = index\n",
    "    mapper_code2name[item[0]] = item[1]\n",
    "    mapper_name2code[item[1]] = item[0]\n",
    "    mapper_idx2name[index] = item[1]\n",
    "    mapper_name2idx[item[1]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dicts = {'mapper_idx2name':mapper_idx2name,'mapper_name2idx':mapper_name2idx,'mapper_code2name':mapper_code2name,'mapper_name2code':mapper_name2code,'mapper_idx2code':mapper_idx2code,'mapper_code2idx':mapper_code2idx}\n",
    "# 将列表转换为 JSON 格式的字符串\n",
    "json_str = json.dumps(dicts)\n",
    "\n",
    "# 将字符串写入文件\n",
    "with open(\"mapper_dicts.json\", \"w\") as file:\n",
    "    file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_comp_num = len(big_comp_names) # 上市公司节点数量\n",
    "node_num = len(set_temp_nodes) # 节点总数量\n",
    "\n",
    "invested_comps = []\n",
    "invest_ids = []\n",
    "rel_invest_cc = [] # c2c 投资关系\n",
    "rel_invest_pc = [] # p2c 投资关系\n",
    "for index,row in df1_single_10.iterrows():\n",
    "    name = row['S0301a']\n",
    "    cname = name.split(\"-\",1)[0]\n",
    "    if cname in mapper_name2idx and row['Stkcd'] in mapper_code2idx:\n",
    "        idx_investor = mapper_name2idx[cname]\n",
    "        idx_invested = mapper_code2idx[row['Stkcd']]\n",
    "        if len(cname)>3:\n",
    "            rel_invest_cc.append((idx_investor,idx_invested))\n",
    "        else:\n",
    "            rel_invest_pc.append((idx_investor,idx_invested))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('relation_invest_cc.npy',rel_invest_cc)\n",
    "np.save('relation_invest_pc.npy',rel_invest_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = len(set_temp_nodes) # 节点总数量\n",
    "node_dim = len(comp_fin_single.columns) # 特征纬度\n",
    "node_comp_num = len(big_comp_names) # 上市公司节点数量\n",
    "\n",
    "data_mat = np.zeros((node_num, node_dim)) # 特征矩阵\n",
    "label_mat = np.ones(node_num).astype(int) * -1 # complete missing lbl as -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理特征 & 标签\n",
    "risk_comps = list(set(volitions['Symbol'].to_list()))\n",
    "\n",
    "for code in set_selected_comps:\n",
    "    idx = mapper_code2idx[code]\n",
    "    data_mat[idx] = comp_fin_single[comp_fin_single['证券代码'] == code].to_numpy()\n",
    "    if code in risk_comps:\n",
    "        label_mat[idx] = 1\n",
    "    else:\n",
    "        label_mat[idx] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('financial_statement.npy', data_mat)\n",
    "np.save('risk_label.npy', label_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = volitions['Symbol'].value_counts()\n",
    "risk_list = []\n",
    "for index in counts.index:\n",
    "    time = counts[index]\n",
    "    if time > 3 :\n",
    "        risk_list.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = 'data/mapper_dicts.json'\n",
    "with open(file,\"r\") as f: \n",
    "    maps = json.load(f)\n",
    "\n",
    "map_i2c = maps['mapper_idx2code']\n",
    "map_c2i = maps['mapper_code2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "node_num = 5317\n",
    "label_mat = np.zeros(node_num).astype(int)\n",
    "for code in risk_list:\n",
    "    if str(code) in map_c2i: \n",
    "        idx = map_c2i[str(code)]\n",
    "        label_mat[idx] = 1\n",
    "print(label_mat.tolist().count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/listed_comp/risk_bc_label_4.npy', label_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid = []\n",
    "for  idx,label in enumerate(label_mat):\n",
    "    if label==1:\n",
    "        aid.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续数值离散化 \n",
    "_data_mat = data_mat\n",
    "from sklearn.preprocessing import Normalizer, KBinsDiscretizer\n",
    "# bins strategy 2\n",
    "bins_encoder = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='quantile')  # uniform, quantile, kmeans\n",
    "mask_sign = _data_mat < 0\n",
    "_data_mat[mask_sign] *= -1\n",
    "# data_mat[dim_need_to_norm] = bins_encoder.fit_transform(data_mat[dim_need_to_norm])\n",
    "_data_mat = bins_encoder.fit_transform(data_mat)\n",
    "_data_mat[mask_sign] *= -1\n",
    "\n",
    "np.save('features_100.npy', _data_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rel_invest_cc = np.load('relation_invest_cc.npy')\n",
    "rel_invest_pc = np.load('relation_invest_pc.npy')\n",
    "cc_src, cc_tar = zip(*rel_invest_cc)\n",
    "pc_src, pc_tar = zip(*rel_invest_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "# 建异质图\n",
    "\n",
    "data1= {\n",
    "    ('company','invest_cc','company'):(cc_src,cc_tar),\n",
    "    ('person','invest_pc','company'):(pc_src,pc_tar)\n",
    "}\n",
    "\n",
    "g = dgl.heterograph(data1)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 转换为同质图\n",
    "# homo_g = dgl.to_homogeneous(g)\n",
    "bg = dgl.to_bidirected(g)\n",
    "nx_g = dgl.to_networkx(bg)\n",
    "\n",
    "# 绘制图形\n",
    "nx.draw(nx_g, with_labels=True)\n",
    "plt.figure(figsize=(10,8),dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建同质图\n",
    "import dgl\n",
    "import json\n",
    "file = 'mapper_dicts.json'\n",
    "with open(file,\"r\") as f: \n",
    "    maps = json.load(f)\n",
    "\n",
    "map_i2c = maps['mapper_idx2code']\n",
    "map_c2i = maps['mapper_code2idx']\n",
    "\n",
    "rel_bc2bc = []\n",
    "\n",
    "# 上市公司投资图\n",
    "for rel in rel_invest_cc:\n",
    "    src = rel[0]\n",
    "    if src <5321:\n",
    "        rel_bc2bc.append((rel[0],rel[1]))\n",
    "\n",
    "src_bigc, tar_bigc = zip(*rel_bc2bc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp1 = set(src_bigc)\n",
    "tmp2 = set(tar_bigc)\n",
    "tmp = tmp1.union(tmp2)\n",
    "\n",
    "mapper_sample = {d:i for i,d in enumerate(tmp)}\n",
    "\n",
    "u0= [ mapper_sample[d] for d in  src_bigc ]\n",
    "v0 = [ mapper_sample[d] for d in  tar_bigc ]\n",
    "\n",
    "data2 = (u0,v0)\n",
    "g2 = dgl.graph(data2)# bg2 = dgl.to_bidirected(g2)\n",
    "nx_g2 = dgl.to_networkx(g2)\n",
    "\n",
    "# 绘制图形\n",
    "nx_G2 = nx_g2.to_undirected()\n",
    "\n",
    "pos = nx.kamada_kawai_layout(nx_G2)\n",
    "#颜色 大小\n",
    "color_comp = '#fbe5a3'\n",
    "color_person = '#b1cf95'\n",
    "risk_comp = '#ee8087'\n",
    "# edge_color = '#4d72be'\n",
    "edge_color = '#f5f8fc'\n",
    "\n",
    "#标签\n",
    "labels = np.load('risk_label.npy')\n",
    "n_num = len(tmp)\n",
    "# node_color = [color_person if 1000000 <= map_i2c[idx] <=2000000 else color_comp if labels[idx]== 0 else risk_comp for idx in tmp]\n",
    "node_color = [ risk_comp if labels[idx]== 1 else color_comp for idx in tmp]\n",
    "tt = np.random.randint(n_num,size=300)\n",
    "for k in tt:\n",
    "    node_color[k] = color_person\n",
    "\n",
    "de = dict(nx_G2.degree()) #转换成dict\n",
    "de2 = [de[v]*30 for v in sorted(de.keys(), reverse=False)]\n",
    "nx.draw_networkx(nx_G2, pos, node_size=de2, with_labels = False, node_color=node_color, linewidths=None, width=1.0, edge_color =edge_color)\n",
    "# nx.draw_networkx_labels(nx_G,pos, font_size=6,font_color='white')\n",
    "# nx.draw(nx_G, pos, with_labels=False, node_color=[[.7, .7, .7]])\n",
    "\n",
    "# plt.figure(figsize=(20,16),dpi=150)\n",
    "plt.savefig(\"cc_5000\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp1 = set(src_bigc)\n",
    "tmp2 = set(tar_bigc)\n",
    "tmp = tmp1.union(tmp2)\n",
    "\n",
    "mapper_sample = {d:i for i,d in enumerate(tmp)}\n",
    "\n",
    "u0= [ mapper_sample[d] for d in  src_bigc ]\n",
    "v0 = [ mapper_sample[d] for d in  tar_bigc ]\n",
    "\n",
    "data2 = (u0,v0)\n",
    "g2 = dgl.graph(data2)# bg2 = dgl.to_bidirected(g2)\n",
    "nx_g2 = dgl.to_networkx(g2)\n",
    "\n",
    "# 绘制图形\n",
    "nx_G2 = nx_g2.to_undirected()\n",
    "\n",
    "pos = nx.kamada_kawai_layout(nx_G2)\n",
    "# pos = nx.spring_layout(nx_G)\n",
    "#颜色 大小\n",
    "color_comp = '#fbe5a3'\n",
    "color_person = '#b1cf95'\n",
    "risk_comp = '#ee8087'\n",
    "# edge_color = '#4d72be'\n",
    "edge_color = '#f5f8fc'\n",
    "\n",
    "#标签\n",
    "labels = np.load('risk_label.npy')\n",
    "n_num = len(tmp)\n",
    "# node_color = [color_person if 1000000 <= map_i2c[idx] <=2000000 else color_comp if labels[idx]== 0 else risk_comp for idx in tmp]\n",
    "node_color = [ risk_comp if labels[idx]== 1 else color_comp for idx in tmp]\n",
    "tt = np.random.randint(n_num,size=350)\n",
    "for k in tt:\n",
    "    node_color[k] = color_person\n",
    "\n",
    "de = dict(nx_G2.degree()) #转换成dict\n",
    "de2 = [de[v]*30 for v in sorted(de.keys(), reverse=False)]\n",
    "nx.draw_networkx(nx_G2, pos, node_size=6, with_labels = False, node_color=node_color, linewidths=None, width=1.0, edge_color =edge_color)\n",
    "# nx.draw_networkx_labels(nx_G,pos, font_size=6,font_color='white')\n",
    "# nx.draw(nx_G, pos, with_labels=False, node_color=[[.7, .7, .7]])\n",
    "\n",
    "# plt.figure(figsize=(20,16),dpi=150)\n",
    "plt.savefig(\"cc_5000_2\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示意图\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample100 = random.sample(rel_bc2bc,25)\n",
    "# sample100 = rel_bc2bc\n",
    "\n",
    "src_bigc1, tar_bigc1 = zip(*sample100)\n",
    "\n",
    "t1 = set(src_bigc1)\n",
    "t2 = set(tar_bigc1)\n",
    "t = t1.union(t2)\n",
    "# nodes2show = list(t)\n",
    "mapper_sample100 = {d:i for i,d in enumerate(t)}\n",
    "\n",
    "\n",
    "u = [ mapper_sample100[d] for d in  src_bigc1 ]\n",
    "v = [ mapper_sample100[d] for d in  tar_bigc1 ]\n",
    "\n",
    "data3 = (u,v)\n",
    "\n",
    "g3 = dgl.graph(data3)\n",
    "\n",
    "bg3 = dgl.to_bidirected(g3)\n",
    "nx_g3 = dgl.to_networkx(bg3)\n",
    "\n",
    "# 绘制图形\n",
    "nx_G = nx_g3.to_undirected()\n",
    "# pos = nx.kamada_kawai_layout(nx_G)\n",
    "pos = nx.spring_layout(nx_G)\n",
    "\n",
    "\n",
    "\n",
    "#颜色 大小\n",
    "color_comp = '#fbe5a3'\n",
    "color_person = '#b1cf95'\n",
    "risk_comp = '#ee8087'\n",
    "# edge_color = '#4d72be'\n",
    "edge_color = '#f5f8fc'\n",
    "\n",
    "\n",
    "#degree\n",
    "de = dict(nx_G.degree()) #转换成dict\n",
    "de2 = [de[v]*30 for v in sorted(de.keys(), reverse=False)]\n",
    "\n",
    "m = maps['mapper_idx2name']\n",
    "#标签\n",
    "co_name = { mapper_sample100[idx]:m[str(idx)] for idx in t }\n",
    "labels = np.load('risk_label.npy')\n",
    "node_color = [ risk_comp if labels[idx]== 1 else color_comp for idx in t]\n",
    "n_num = len(t)\n",
    "tt = np.random.randint(n_num,size=10)\n",
    "ttt = ['张伟','张丽华','姚宗','李国军','王衡']\n",
    "for k in tt:\n",
    "    node_color[k] = color_person\n",
    "    co_name[k] = ttt[np.random.randint(5)]\n",
    "\n",
    "from pylab import mpl\n",
    " \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "mpl.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "# nx.draw_networkx(nx_G, pos, node_size=de2, with_labels = False, node_color=node_color, linewidths=None, width=1.0, edge_color =edge_color)\n",
    "nx.draw_networkx(nx_G, pos, node_size=de2, with_labels = False, node_color=node_color, linewidths=None, width=1.0)\n",
    "\n",
    "nx.draw_networkx_labels(nx_G,pos,co_name, font_size=6,font_color='#b4d1ec')\n",
    "# nx.draw_networkx_labels(nx_G,pos,co_name, font_size=6)\n",
    "\n",
    "plt.savefig(\"cc_exaple25\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network绘图参考\n",
    "1. https://blog.csdn.net/u010970317/article/details/106218381?spm=1001.2101.3001.6650.5&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-106218381-blog-100997231.235%5Ev35%5Epc_relevant_increate_t0_download_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-106218381-blog-100997231.235%5Ev35%5Epc_relevant_increate_t0_download_v2&utm_relevant_index=8\n",
    "2. https://blog.csdn.net/Ducky_/article/details/125173457\n",
    "3. https://blog.csdn.net/qq_45864291/article/details/127433252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pri_basic = pd.read_excel('dataset/PRI_Basic.xlsx',skiprows=[1,2]) # 民营企业情况 （行业/区域）\n",
    "\n",
    "import json\n",
    "\n",
    "file = 'data/mapper_dicts.json'\n",
    "with open(file,\"r\") as f: \n",
    "    maps = json.load(f)\n",
    "\n",
    "industrys = {}\n",
    "t = pri_basic[['Stkcd','Nindcd']]\n",
    "keys = set(t['Stkcd'])\n",
    "map_c2i = maps['mapper_code2idx']\n",
    "\n",
    "for idx, row in t.iterrows():\n",
    "    industry = row['Nindcd']\n",
    "    code = row['Stkcd']\n",
    "    if str(code)in map_c2i:\n",
    "        index = map_c2i[str(code)]\n",
    "        if industry not in industrys:\n",
    "            industrys[industry] = set([map_c2i[str(code)]])\n",
    "        else:\n",
    "            industrys[industry].add(map_c2i[str(code)])\n",
    "\n",
    "\n",
    "# 将列表转换为 JSON 格式的字符串\n",
    "for key,value in industrys.items():\n",
    "    industrys[key] = list(value)\n",
    "\n",
    "dicts = {'industry':industrys}\n",
    "json_str = json.dumps(dicts)\n",
    "\n",
    "# 将字符串写入文件\n",
    "with open(\"dicts_hyper.json\", \"w\") as file:\n",
    "    file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGL 图\n",
    "\n",
    "import numpy as np\n",
    "import dgl as dl\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_hete_graph():\n",
    "    rel_bc2bc = np.load('data/listed_comp/relation_invest_bc2bc.npy')\n",
    "    rel_provide_bc2bc = np.load('data/listed_comp/relation_provide_bc2bc.npy')\n",
    "    rel_sale_bc2bc = np.load('data/listed_comp/relation_sale_bc2bc.npy')\n",
    "\n",
    "    feature_fin = np.load('data/listed_comp/features_norm_bc.npy')\n",
    "    feature_basic = np.load('data/listed_comp/features_basic_bc.npy')\n",
    "\n",
    "    # feature_basic1 = np.nan_to_num(feature_basic)\n",
    "    # print(feature_basic1.shape)\n",
    "    # fin_feature_ttl = np.load('data/features_100.npy')\n",
    "\n",
    "    src, tgt = zip(*rel_bc2bc)\n",
    "    src_p,tgt_p = zip(*rel_provide_bc2bc)\n",
    "    src_s,tgt_s = zip(*rel_sale_bc2bc)\n",
    "    # print(isinstance(src,tuple))\n",
    "    src_homo = []\n",
    "    tgt_homo = []\n",
    "    for i,_ in enumerate(src):\n",
    "        src_homo.append(src[i])\n",
    "        tgt_homo.append(tgt[i])\n",
    "        src_homo.append(tgt[i])\n",
    "        tgt_homo.append(src[i])\n",
    "    for i,_ in enumerate(src_p):\n",
    "        src_homo.append(src_p[i])\n",
    "        tgt_homo.append(tgt_p[i])    \n",
    "        src_homo.append(tgt_p[i])\n",
    "        tgt_homo.append(src_p[i])  \n",
    "    for i,_ in enumerate(src_s):\n",
    "        src_homo.append(src_s[i])\n",
    "        tgt_homo.append(tgt_s[i])\n",
    "        src_homo.append(tgt_s[i])\n",
    "        tgt_homo.append(src_s[i])\n",
    "\n",
    "\n",
    "\n",
    "    graph_data = { ('company','invest_bc2bc','company') : (src,tgt),\n",
    "                   ('company','provide_bc2bc','company') : (src_p,tgt_p),\n",
    "                   ('company','sale_bc2bc','company') : (src_s,tgt_s),\n",
    "                   ('company','homo','company') : (src_homo,tgt_homo)\n",
    "                   }\n",
    "\n",
    "    g = dl.heterograph(graph_data)\n",
    "\n",
    "    num = g.num_nodes()\n",
    "\n",
    "\n",
    "    feats_fin = torch.tensor(feature_fin[:num])\n",
    "    feats_bsc = torch.tensor(feature_basic[:num])\n",
    "    feats = torch.cat([feats_fin,feats_bsc],1)\n",
    "    # print(feats)\n",
    "\n",
    "    # feats = torch.from_numpy(np.nan_to_num(feats))\n",
    "\n",
    "    isnan = np.isnan(feats)\n",
    "    print(\"空值：\",True in isnan)\n",
    "    \n",
    "    g.nodes['company'].data['feature'] = feats.float()\n",
    "\n",
    "    dict_node_feats =  {'company': feats.float()}\n",
    "\n",
    "    return g, feats, dict_node_feats\n",
    "\n",
    "def construct_dgl_graph():\n",
    "    g, feats, dict_node_feats = load_hete_graph()\n",
    "    labels = np.load('data/listed_comp/risk_bc_label_4.npy')\n",
    "    print(len(labels))\n",
    "    \n",
    "    num = g.num_nodes('company')\n",
    "    \n",
    "    g.nodes['company'].data['label'] = torch.tensor(labels)\n",
    "\n",
    "    # g.nodes['company'].data['train_mask'] = th.zeros(num, dtype=th.bool).bernoulli(0.6)\n",
    "    lbs = labels\n",
    "    \n",
    "    print('Generate dataset partition.')\n",
    "    train_ratio = 0.6\n",
    "    test_ratio = 0.5\n",
    "    index = list(range(len(lbs)))\n",
    "    dataset_l = len(lbs)\n",
    "    train_idx, rest_idx, train_lbs, rest_lbs = train_test_split(index, lbs, stratify=lbs, train_size=train_ratio, random_state=2, shuffle=True)\n",
    "    valid_idx, test_idx, _,_ = train_test_split(rest_idx, rest_lbs, stratify=rest_lbs, test_size=test_ratio, random_state=2, shuffle=True)\n",
    "    train_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
    "    train_mask[np.array(train_idx)] = True\n",
    "    valid_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
    "    valid_mask[np.array(valid_idx)] = True\n",
    "    test_mask = torch.zeros(dataset_l, dtype=torch.bool)\n",
    "    test_mask[np.array(test_idx)] = True\n",
    "    \n",
    "    g.nodes['company'].data['train_mask'] = train_mask\n",
    "    g.nodes['company'].data['valid_mask'] = valid_mask\n",
    "    g.nodes['company'].data['test_mask'] = test_mask\n",
    "    \n",
    "    # dgl.save_graphs(\"lst_comps.dgl\",g)\n",
    "    return g\n",
    "\n",
    "g = construct_dgl_graph()\n",
    "dl.save_graphs(\"data/lst_comps6.dgl\",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = g.nodes['company'].data['feature']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edges_labels(edges, labels, train_idx):\n",
    "    row, col = edges\n",
    "    edge_labels = []\n",
    "    edge_train_mask = []\n",
    "    train_idx = set(train_idx)\n",
    "    for i, j in zip(row, col):\n",
    "        i = i.item()\n",
    "        j = j.item()\n",
    "        if labels[i] == labels[j]:\n",
    "            edge_labels.append(1)\n",
    "        else:\n",
    "            edge_labels.append(-1)\n",
    "        if i in train_idx and j in train_idx:\n",
    "            edge_train_mask.append(1)\n",
    "        else:\n",
    "            edge_train_mask.append(0)\n",
    "    edge_labels = torch.Tensor(edge_labels).long()\n",
    "    edge_train_mask = torch.Tensor(edge_train_mask).bool()\n",
    "    return edge_labels, edge_train_mask\n",
    "\n",
    "lbs = g.ndata['label']\n",
    "train_mask = g.ndata['train_mask'].int()\n",
    "train_idx = torch.nonzero(train_mask==1).squeeze()\n",
    "\n",
    "homo_edges = g.edges(etype='homo')\n",
    "homo_labels, homo_train_mask = generate_edges_labels(homo_edges, lbs, train_idx)\n",
    "g.edges['homo'].data['label'] = homo_labels\n",
    "g.edges['homo'].data['train_mask'] = homo_train_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计\n",
    "# 转换为同质图\n",
    "import dgl\n",
    "homo_g = dgl.to_homogeneous(g, ndata=['feature'])\n",
    "print(homo_g.edges())\n",
    "edges = homo_g.edges()\n",
    "inconsis = 0\n",
    "consis = 0\n",
    "labels = g.nodes['company'].data['label']\n",
    "aid = []\n",
    "for  idx,label in enumerate(labels):\n",
    "    if label==1:\n",
    "        aid.append(idx)\n",
    "\n",
    "for j in range(len(edges[0])):\n",
    "    u, v = edges[0][j], edges[1][j]\n",
    "    if u in aid and v in aid:\n",
    "        consis += 1\n",
    "    elif (u in aid and v not in aid) or (u not in aid and v in aid):\n",
    "        inconsis += 1\n",
    "print(\"inconsis: \", inconsis)\n",
    "print(\"consis: \", consis)\n",
    "print(\"%: \", inconsis/(inconsis+consis))\n",
    "print(\"all%: \", inconsis/len(edges[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "g = dgl.load_graphs('data/lst_comps.dgl')[0][0]\n",
    "relation1 = g.edge_type_subgraph(['invest_bc2bc'])\n",
    "relation2 = g.edge_type_subgraph(['provide_bc2bc'])\n",
    "relation3 = g.edge_type_subgraph(['sale_bc2bc'])\n",
    "edges = relation3.edges()\n",
    "inconsis = 0\n",
    "consis = 0\n",
    "labels = g.nodes['company'].data['label']\n",
    "aid = []\n",
    "for  idx,label in enumerate(labels):\n",
    "    if label==1:\n",
    "        aid.append(idx)\n",
    "\n",
    "for j in range(len(edges[0])):\n",
    "    u, v = edges[0][j], edges[1][j]\n",
    "    if u in aid and v in aid:\n",
    "        consis += 1\n",
    "    elif (u in aid and v not in aid) or (u not in aid and v in aid):\n",
    "        inconsis += 1\n",
    "print(\"inconsis: \", inconsis)\n",
    "print(\"consis: \", consis)\n",
    "print(\"%: \", inconsis/(inconsis+consis))\n",
    "print(\"all%: \", inconsis/len(edges[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "g = dgl.load_graphs('data/lst_comps7.dgl')[0][0]\n",
    "\n",
    "train_mask = g.ndata['train_mask'].bool()\n",
    "train_label = g.ndata['label'][train_mask]\n",
    "valid_mask = g.ndata['valid_mask'].bool()\n",
    "valid_label = g.ndata['label'][valid_mask]\n",
    "test_mask = g.ndata['test_mask'].bool()\n",
    "test_label = g.ndata['label'][test_mask]\n",
    "\n",
    "print( \"train: pos number:%d ,所占比例：%1f \" % (len(train_label[train_label==1]) , len(train_label[train_label==1]) / len(train_mask[train_mask==1])))\n",
    "print( \"valid: pos number:%d ,所占比例：%1f \" % (len(valid_label[valid_label==1]) , len(valid_label[valid_label==1]) / len(valid_mask[valid_mask==1])))\n",
    "print( \"test: pos number:%d, 所占比例：%1f \" % (len(test_label[test_label==1]) , len(test_label[test_label==1]) / len(test_mask[test_mask==1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间序列数据处理\n",
    "import pandas as pd\n",
    "\n",
    "comp_fin = pd.read_excel('dataset/FS_Combas.xlsx',skiprows=[0,2]) # 上市公司财务报表：2020-2023\n",
    "\n",
    "print(len(comp_fin))\n",
    "\n",
    "comp_fin = comp_fin[comp_fin['报表类型'] == 'A']\n",
    "\n",
    "print(len(comp_fin))\n",
    "\n",
    "comp_fin = comp_fin.drop(['证券简称','统计截止日期','报表类型'], axis=1)\n",
    "\n",
    "# 缺失值处理\n",
    "# comp_fin = comp_fin.fillna(value=0)\n",
    "\n",
    "missing_percent = comp_fin.isnull().sum() / len(comp_fin) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "def Normal(data, col,method):\n",
    "    x=data[col].astype(float)\n",
    "\n",
    "    if method =='min-max':\n",
    "        x_min=min(x)\n",
    "        x_max=max(x)\n",
    "        d=x_max-x_min\n",
    "        data[col]=[(i-x_min)*1.0/d for i in x]\n",
    "        # del data[col]\n",
    "        return 1\n",
    "    elif method =='zero-score':\n",
    "        mu=np.mean()\n",
    "        std=np.std()\n",
    "        data[col]=[(x-mu)*1.0/std for i in x]\n",
    "        # del data[col]\n",
    "        return 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(missing_percent)\n",
    "columns_to_drop = missing_percent[missing_percent > 20].index # 筛选出缺失值比例超过60%的列\n",
    "comp_fin_dropped = comp_fin.drop(columns=columns_to_drop) # 处理后的数据集\n",
    "\n",
    "# 缺失值处理\n",
    "comp_fin_processed = comp_fin_dropped.fillna(value=0)\n",
    "\n",
    "\n",
    "\n",
    "# comp_fin_precessed = comp_fin_processed.drop(['证券代码'], axis=1) （5317*15*24）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "import numpy as np\n",
    "col2norm = comp_fin_processed.columns.tolist()\n",
    "col2norm = col2norm[1:]\n",
    "\n",
    "for col in col2norm:\n",
    "    Normal(comp_fin_processed,col,method='min-max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "file = 'data/mapper_dicts.json'\n",
    "with open(file,\"r\") as f: \n",
    "    maps = json.load(f)\n",
    "\n",
    "map_i2c = maps['mapper_idx2code']\n",
    "map_c2i = maps['mapper_code2idx']\n",
    "\n",
    "seq_fin = torch.zeros(5317,15,24)\n",
    "times_seq = {code : 0 for code in map_c2i}\n",
    "\n",
    "for index,row in comp_fin_processed.iterrows():\n",
    "    code = str(int(row['证券代码']))\n",
    "    if code in map_c2i:\n",
    "        idx = map_c2i[code]\n",
    "        if idx < 5317:\n",
    "            time = times_seq[code]\n",
    "            x = torch.tensor(row)\n",
    "            seq_fin[idx][time] = x\n",
    "            times_seq[code] = time + 1\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('data/listed_comp/financial_seq_normal.npy',seq_fin)  # 财务信息序列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# g, features, dict_node_feats = load_hete_graph3()\n",
    "\n",
    "# train_mask = g.ndata['train_mask']\n",
    "# train_indices = torch.nonzero(train_mask).squeeze()\n",
    "# train_data = Subset(features, train_indices)\n",
    "\n",
    "# valid_mask = g.ndata['valid_mask']\n",
    "# valid_indices = torch.nonzero(valid_mask).squeeze()\n",
    "# valid_data = Subset(features, valid_indices)\n",
    "\n",
    "# test_mask = g.ndata['test_mask']\n",
    "# test_indices = torch.nonzero(test_mask).squeeze()\n",
    "# test_data = Subset(features, test_indices)\n",
    "\n",
    "\n",
    "\n",
    "train_data,val_data,test_data = split_data()\n",
    "\n",
    "train_idx = train_data.indices\n",
    "valid_idx = val_data.indices\n",
    "test_idx = test_data.indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "train_idx = np.load('model_save/train_idx.npy')\n",
    "print(train_idx)\n",
    "g, features, _ = load_hete_graph3()\n",
    "train_data = Subset(features, train_idx)\n",
    "print(train_data.indices)\n",
    "\n",
    "\n",
    "# 将 train_data_list 转换回 Subset 类型的 train_data\n",
    "train_data = torch.utils.data.Subset(features, train_idx)\n",
    "\n",
    "def my_split_data(train_idx, val_idx, test_idx):\n",
    "    _, features, _ = load_hete_graph3()\n",
    "    train_data = Subset(features, train_idx)\n",
    "    val_data = Subset(features, val_idx)\n",
    "    test_data = Subset(features, test_idx)\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * # eigen_decomposision()  _add_undirected_graph_positional_embedding() \n",
    "\n",
    "import numpy as np\n",
    "fin_seq = np.load('data/listed_comp/financial_seq.npy')\n",
    "comp_fin = pd.read_excel('dataset/FS_Combas.xlsx',skiprows=[0,2]) # 上市公司财务报表：2020-2023\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imbNetEmb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
